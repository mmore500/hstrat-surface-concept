\section{Steady Algorithm} \label{sec:steady}

The steady criterion seeks to retain data items from time points evenly spread across observed history.
The criterion can be formulated as minimization of the largest gap size between retained data items.
For a buffer size $\colorS$ and time elapsed $\colorT$, largest gap size can be minimized at best $\left\lceil \colorT / \colorS \right\rceil$.
This section presents a stream curation algorithm designed to support the steady criterion, guaranteeing maximum gap size no worse than $2\left\lceil \colorT / \colorS \right\rceil$.

\subsection{Steady Algorithm Strategy}
\label{sec:steady-strategy}

Figure \ref{fig:hanoi-intuition-steady} overviews the proposed algorithm's core strategy, which revolves around prioritizing data item retention according to their sequence index hanoi value $\colorH(\colorTbar)$.
Specifically, we aim to keep data items with the largest hanoi values.

It turns out that with all data items $\colorH(\colorTbar) > n$ retained, gap size is at most $\colorg \leq 2^n - 1$.
To understand, imagine discarding items with $\colorH(\colorTbar) = 0$.
This action would drop every other item, and increase gap size from 0 to $\colorg \leq 1$.
Then, removing items with $\colorH(\colorTbar) = 1$ would again drop every other item, and increase gap size to $\colorg \leq 3$.
Continuing this pattern to prune successive hanoi values provides well-behaved transitions gradually increasing gap size while maintaining even spacing.

We thus define our algorithmic goal as maintaining, for a ratcheting threshold $n(\colorT)$, all items $\colorH(\colorTbar) > n(\colorT)$ for some threshold $n(\colorT)$.
Formally,
\begin{align*}
\textsf{goal\_steady}
\coloneq \{
\colorTbar \in [0 \twodots \colorTbar]
: \colorH(\colorTbar) > n(\colorT)
\}.
\end{align*}
In practice, this requires repeatedly discarding all items with lowest hanoi value $\colorH(\colorTbar) = n(\colorT)$ as ingests elapse.

To fill available buffer space $\colorS$, we will show that $n(\colorT) = \colort - 1$.

\input{thm/steady-hv-geq-epoch}

\subsection{Steady Algorithm Mechanism}
\label{sec:steady-mechanism}

\input{fig/hsurf-steady-intuition}

Maintaining all $\colorTbar$ such that $\colorH(\colorT) \geq \colort$, Lemma \ref{thm:steady-hv-geq-epoch} shown, uses nearly all buffer space $\colorS$.%
\footnote{%
Although we, in fact, have one extra buffer site left over, in practice, it is often convenient to use this site to permanently retain the very first or the very most recent data item.%
}
% Note that, under this scheme, there are never retained items $\colorT$ such that $\colorH(\colorT) < \colort - 1$.
Thus, under our scheme, each epoch, all items with $\colorH(\colorT) = \colort - 1$ must be overwritten to make space for new items with \hv{} $\colorh \geq \colort$.

Figure \ref{fig:hsurf-steady-intuition} overviews the layout procedure used to orchestrate replacement of data items with \hv{} $\colort - 1$ each epoch.
This procedure divides buffer space into ``bunches,'' themselves divided into ``segments.''
Bunch 0 contains one segment of length $\colors$ sites.
Then, for $n > 0$, bunch $n$ contains $2^{n-1}$ segments.
Although segment count increases across bunch, we define segment length to decrease by 1 each bunch.
So, segments in the last bunch contain only one site.
With $\colors$ bunches, available buffer space $\colorS$ is nearly filled,
\begin{align*}
\colors + \sum_{i=0}^{\colors-1} (\colors - i - 1) \times 2^{i} = 2^{\colors} - 1 = \colorS - 1.
\end{align*}

If, for a hanoi value $\colorh$, we place one data item $\colorH(\colorTbar) = \colorh$ per segment, the proposed layout scheme happens to naturally segregate data items with that \hv{} by ingestion epoch across bunches.
Bunch 0 will contain the first data item with \hv{} $\colorh$, which is encountered in epoch $\colort=\colorh - \colors + 1$.
Bunch 1 contains the data item with that \hv{} $\colorh$ from epoch $\colort=\colorh - \colors + 2$, bunch 2 contains the two data items $\{ \colorTbar \in \colortsetofT - \colors + 2 : \colorH(\colorTbar) \}$, and so forth.
Because only one not-yet-encountered \hv{} surfaces each epoch, during any one epoch we will be taking data item instances from only the top $\colors$ \hv{}'s $\colorh \geq \colort$ into storage.
Segment size (decreasing by one each bunch) is arranged so that one instance of all $\colors - n$ \hv{}'s $\colorh$ that have ``progressed'' to bunch $n$ can be stored within each segment in that bunch.

\input{fig/hsurf-steady-implementation}

The properties of this arrangement become useful in managing elimination of data items with \hv{} $\colorh$ during epoch $\colort=\colorh + 1$.
We have that \hv{} $\colorh + \colors$ will place one data item in bunch 0 during epoch $\colort=\colorh + 1$.
This is the same number of data items left by \hv{} $\colorh$ in bunch 0.
The same holds for all bunches, with data items left by \hv{} $\colorh$ equivalent in number to those to be placed in bunch $n$ from \hv{} $\colorh + \colors - n$ during epoch $\colort = \colorh + 1$.

As shown in Figure \ref{fig:hsurf-steady-implementation}, we can take advantage of this one-to-one correspondence between incoming data items and data items of \hv{} $\colorh=\colort-1$ to choreograph clean elimination of \hv{} $\colorh$ by overwrites each epoch.
In determining placement site $\colork$ for ingest $\colorTbar$, we map incoming data items with \hv{} $\colorh \geq \colort$ over items $\colorh = colort - 1$ slated for elimination by placing them at segment positions $\colorh$ modulus segment size.
Calculating the number of \hv{} instances $\colorh = \colorH(\colorTbar)$ already seen $\colorTbar < \colorT$ identifies the segment where data item $\colorTbar$ should be placed.

\input{thm/steady-hv-elimination}

We refer the reader to our supplemental Python-language implementation for an exact step-by-step listing of this procedure, which comprises a handful of fast $\mathcal{O}(1)$ binary operations (e.g., bit mask, bit shift, count leading zeros).
Also implemented fully in accompanying materials, ingestion time calculation for lookup of the data item at $\colork$ at time $\colorT$ boils down to decoding its segment/bunch indices and checking whether (if slated) it has yet been replaced during the current epoch $\colort$.
For buffer size $\colorS$, ingestion times for all data items can calculated with ideal time complexity $\mathcal{O}(\colorS)$.

\subsection{Steady Algorithm Criterion Satisfaction}
\label{sec:stready-satisfaction}

In this final subsection, we establish an upper bound on gap size $\colorg$ for a buffer of size $\colorS$ at time $\colorT$ under the proposed steady curation algorithm.
Figure \ref{fig:hsurf-steady-implementation-satisfaction} plots an example of actual worst gap size over time under this algorithm.

\input{thm/steady-gap-size}
