\section{Conclusions and Further Directions} \label{sec:conclusion}

In closing, we will briefly review principal objectives and major results of presented work, then highlight potential impact of these contributions.
We finish by laying out future work --- in yet-incomplete aspects of the presented work as well as opportunities for extension and elaboration.
We also outline steps to build out broad availability of developed algorithms as an off-the-shelf, plug-and-play software tool.

\subsection{Summary and Discussion}

In this work, we have developed new strategies for fast and space-efficient ``data stream curation'' --- subsampling from a rolling sequence of data items to dynamically maintain a representative cross-sample across observed time points.
Our approach, in particular, targets use cases that are fixed-capacity and resource-constrained.

As a simplifying assumption, we have reduced data ingestion to a sole update operation: ``site selection,'' picking a buffer index to write the $n$th received data item into --- overwriting any existing data item at that location.
In the interest of concision and efficiency, we forgo any explicit metadata storage or data structure overhead (e.g., pointers).
Instead, we require site selection for the $n$th ingested item to be computable \textit{a priori}.
Interpereting stored data, therefore, additionally requires support for ``inverse'' decoding of provenance based solely on an item's buffer index the number of items that have been ingested from the data stream.

Ultimately, the purpose of stream curation is to dictate what data to keep, and for how long.
As objectives in this regard depend heavily on use case, we have explored a suite of three possible retention strategies.

The first is \textit{steady} curation, which calls for retention of evenly-spaced samples across data stream history.
Our proposed algorithm guarantees worst-case even coverage within a factor of two of optimal.

The second two curation objectives explored --- \textit{stretched} and \textit{tilted} criteria --- bias retention to favor earlier or more recent data items, respectively.
Proposed algorithms for these two criteria relate closely in structure, differing only in that the former freezes first encountered data items in place, while the latter uses a ring buffer approach to maintain the most-recently encountered data items.
Unlike the proposed steady curation algorithm, which handles indefinitely many data item ingestions, we leave behavior for time $\colorT \geq 2^{\colorS}$ unspecified in defining the proposed stretched and tilted algorithms.

All three algorithms provide formal description of buffer layout procedure, and indicate how site selection proceeds on this basis.
As implemented, all algorithms provide $\mathcal{O}(1)$ site selection operations and are $\mathcal{O}(\colorS)$ to decode ingest times at all $\colorS$ buffer sites.
Presentation for each constructs strict worst-case upper bounds on curation quality across elapsed data item ingests.

\subsection{Future Algorithm Development}

The core limitation of present work, as mentioned above, is that the stretched and tilted algorithms are only specified up to $2^{\colorS}$ data item ingests.
As such, work remains to design behavior past this point.
One possibility is switching over at $\colorT = 2^{\colorS}$ to steady curation on data item hanoi value $\colorH(\colorT)$ (i.e., rather than on $\colorT$ itself).

Another enhancement would be random-access lookup calculation.
Whereas current implementation is $\mathcal{O}(1)$ under the steady algorithm, random access time complexity currently depends on buffer size $\colorS$ for stretched and tilted curation.

In addition, several interesting openings exist for extension of additional operations on curated data.
Notably, fast retrieval of the retained data item closest to a query $\colorTbar$ and fast ingest-order iteration over buffer sites $\colork \in \colorS$ would be useful.

A final unexplored possibility is fast comparison between curated collections --- critical for applications that rely on identification of discrepancies between stream histories, such as hereditary stratigraphy.
For this purpose, fast operations to identify common retained time points $\colorTbar$ between records differing in elapsed ingest count $\colorT$ would be valuable.
The stable buffer position of data items over their retention raises the possibility of applying vectorized operations for this purpose (e.g., masked bitwise equality tests).

% of comparing two records to find the first mismatch.
% This, in the context of hereditary stratigraphy in bounding the most recent common ancestor (MRCA) of two lineages.
%Operations that consolidate data through join operations rather than deletion also remain an interesting unaddressed extension.

\subsection{Algorithm Implementation}

Ultimate motivation of this work lies in putting efficient stream curation procedures into production to solve critical data management challenges.
Indeed, a driving application of this work is to support distributed lineage history tracking in large-scale digital evolution experiments.
In this application, stream curation is used to downsample randomly-generated lineage ``checkpoints'' that accrue as generations elapse, with divergence between two lineages identifiable via mismatching checkpoints \citep{moreno2022hereditary}.
Indeed, prototype implementations of presented algorithms have already been successfully applied to support lineage tracking over massively-distributed, agent-based evolution experiments conducted on the 850,000 core Cerebras Wafer-Scale Engine (WSE) device \citep{moreno2024trackable}.
Promisingly, empirical microbenchmark experiments reported in that work corroborate order-of-magnitude efficiency gains from the algorithms presented here, compared to existing approaches.

This said, we anticipate broader utility of presented stream curation algorithms beyond this original domain in lineage tracking.
This possibility has motivated our strategy to pursue standalone formalization, as presented here.
Another key aspect of this strategy, however, is also standalone software implementations of algorithms proposed herein.
As described in Section \ref{sec:materials}, we have organized stream-curation specific components --- including all three algorithms presented here --- as a standalone software library \citep{moreno2024downstream}.
Going forward, we intend for stream curation algorithms to support lineage tracking implementation as an public-facing external dependency rather than as an opaque internal utility \citep{moreno2022hstrat}

Numerous challenges remain in preparing library code to meet user needs.
One issue is in cross-language support.
Partial implementations are currently available in Python, Zig, and the closely-related Cerebras Software Language (CSL) \citep{moreno2024hsurf,moreno2024downstream,moreno2024wse}.
For our own purposes, we plan to port the stream curation algorithms to Rust and C++ in the near future, and publish a crate/header-only library, respectively, as well.

We would be highly interested in collaborating with interested outside developers in assembling implementations in other languages as needed --- whether that's folding new implementations into the \texttt{downstream} software repository or simply linking to outside repositories with additional implementations from the central hub documentation.
In either case, care will need to be taken to precisely match exact details across implementations of the same stream curation algorithm, as  metadata is implicit and the semantics of stored data, therefore, depends subtly upon the exact site selection implementation used.
One possible approach to this issue would simply be designation of a canonical implementation and developmentof platform-agnostic means to validate other implementations against.
Alternately, effort could be invested in preparing and maintaining an explicit standard or specification.
