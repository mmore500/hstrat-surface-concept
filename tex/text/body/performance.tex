\section{Experimental Evaluation}
\label{sec:performance}

\input{fig/memory}
\input{fig/speed}


To better understand the generalized ring buffer approach's runtime performance characteristics, we undertook on-hardware benchmark tests to compare trade-offs in memory usage and stream processing speed against alternate approaches.
In addition to the proposed tilted algorithm, our tests also considered Gunther's compressing circular buffer algorithm as, to our knowledge, its performance has only been reported for a desktop computing context \citep{gunther2014algorithm}.

Given the focus of this work in supporting data stream management resource-constrained computing scenarios, we targeted our benchmarks to the Raspberry Pi Pico RP2040 --- a popular ARM-based microcontroller platform \citep{raspberrypico2024}.
Due to fundamental tradeoffs in processor architecture related to manufacturing cost and energy intensivity, embedded devices exhibit a substantially differentiated performance profile compared to mainline desktop CPU processors \citep{schlett2000embedded}.
Notably, embedded microprocessors typically offer more limited faculties for pipelining, vector operations, and specialized instruction logic  \citep{yiu2015definitive}
Indeed, some similarities exist in this regard between embedded processors and emerging AI/ML hardware accelerator platforms' worker compute cores in terms of the reduced instruction set size and low relative latency for local memory access --- although the latter much more heavily emphasizes instruction-level parallelism (i.e., vectorized operations)
\citep{lie2023cerebras} \citep{vasiljevic2021compute}.

In our benchmark trials, we measured time elapsed to ingest 10,000, 100,000, and 1,000,000 items from a data stream.
As a simple stand-in for otherwise domain-specific use cases, the underlying data stream in our experiments comprised a deterministic sequence generated by 32-bit xorshift PRNG, where each element is related to the prior by three shift/xor operations \citep{marsaglia2003xorshift}.
Being nontrivial, this stream model ensured the performance of a verifiable computation while remaining straightforward, lightweight, and reproducible.

Across trials, we tested algorithm behavior in processing single-bit, single-byte, word, and double-word data items drawn from the xorwow stream.
Within each trial, algorithms were configured to perform online downsampling via thinning within a designated item capacity.
\footnote{%
Although we focus on downsampling via thinning here, a number of other mechanisms for summarization of stream history are possible, as discussed in greater detail elsewhere in the text.
}
Tests employed buffer capacities of 64, 256, 1,024, or 4,096 items.

\subsection{Compared Approaches and Control Treatments}

\input{fig/surface-control-tilted}
\input{alg/control-doubling-steady}
\input{alg/control-doubling-tilted}
\input{alg/zhao-tilted-full}

\input{tab/algorithms.tex}

Our benchmark considers three primary algorithm categories, comparing (1) naive doubling approaches, (2) stateful, iterative approaches, or (3) a generalized ring buffer approach.

A pair of algorithms was fielded within each algorithm category: one providing steady retention and one providing tilted retention.
(Recall that steady and tilted retention differ with respect to relative prioritization of newer versus older data, as shown in Figure \ref{fig:criteria-intuition}).
Given that the appropriate choice between these two retention patterns differs fundamentally between use cases, our focus lies in comparisons among steady algorithms or among tilted algorithms, with respect to speed, memory use, and curation quality.

For the generalized ring buffer approach, we included the steady-spaced compressing circular buffer algorithm proposed in \citet{gunther2014algorithm} and the DStream algorithm for tilted retention introduced in this work (Listing \ref{lst:tilted-site-selection}) --- respectively, denoted as ``gunther steady'' and ``dstream tilted.''
In implementing these generalized ring buffer approaches, bookkeeping state was limited to a single ingest counter.
One notable trade-off associated with these approaches is the cost of calculations required to calculate a selected storage site for ingested data.

Compare against a standard approach to the problem, along the lines of the generalized dimension-reduction framework proposed by \citet{zhao2005generalized}.
An attractive aspect of these algorithms is their concise principle of operation.
Retained items are stored within a single list, sorted by arrival time.
Gap size between neighboring items is also recorded.
Upon arrival, new items are appended at the tail of the item list.
Under the ``equi-segmented'' (i.e., steady) algorithm, the smallest pair of neighboring segments are consolidated upon reaching a maximum specified storage capacity.
Conversely, for the ``vari-segmented'' (i.e., tilted) algorithm, consolidation targets the most recent pair of identically-sized segments.
We denote these algorithms as ``zhang steady'' and ``zhang tilted.''
One notable trade-off of these approaches is memory use overhead from storing segment size information with each retained value.

Finally, we extend our consideration to also include a more ``obvious'' approach in which an intermittent clean-up procedure is applied to over data items accumulated within a contiguous buffer.
This approach, described in \citet{gunther2014algorithm}, simply strips away every second element upon reaching buffer capacity.
For steady retention, the sampling interval at which data items arrive is doubled each fill cycle.
As a commensurate analog approximating tilted retention, we  also included a variant of this procedure where the sampling interval is kept constant.
As such, larger gap size accumulates among older retained data items.
We term these two approaches as ``doubling steady'' and ``doubling tilted,'' respectively.
One notable trade-off associated with these approaches is in fluctuating, and often partial, use of available buffer space.

To further contextualize our experiments, we included two additional benchmarked operations as control treatments.
For our ``discard-only'' control, stream data was generated as per usual, but none was stored.
In turn, the ``simple ringbuf'' control stored data within a conventional circular buffer.

Table \ref{tab:algorithms} overviews algorithms and control treatments included in our experiments.

\subsection{Benchmark Methodology}

Experiments were performed on a Raspberry Pi Pico RP2040 Microcontroller Board, which features a Dual-Core ARM Cortex-M0+ chip clocked at 133 MHz.
The RP2040 provides 264KB of SRAM and 2MB of onboard flash memory.
Experiment code was written with C++ and compiled with the Raspberry Pi Pico SDK version 2.1.0, which bundles ARM GNU Toolchain 13.3.1 \citep{raspberrypipico2024}.

Timings were taken using the SDK's included \texttt{std::chrono::high\_resolution\_clock} implementation, with ten replicates per timing.
Memory usage was calculated through a combination of compile-time inspection of type footprints via the \texttt{sizeof} builtin and runtime inspection of container occupancy.
For the doubling algorithms, which fluctuate between utilizing half of buffer space and full buffer space, we assumed the mean case of 75\% buffer utilization.

Across all trials, executables were compiled with optimization level \texttt{O3} and native micro-architecture enabled.
Appropriate steps were taken to prevent operations of interest from being inadvertently optimized away.
To accurately reflect scenarios where stream length is unknown \textit{a priori}, we took steps to ensure that total loop count was unknown as compile time.
However, we assumed available buffer capacity to be designated at compile time.
Benchmarking code and results are publicly available, as described below in Section \ref{sec:materials}.

\subsection{Software and Data Availability}
\label{sec:materials}

Supporting software and executable notebooks for this work are available via Zenodo at \url{https://doi.org/10.5281/zenodo.10779240} \citep{moreno2024hsurf}.
DStream algorithm implementations are also published on PyPI in the \texttt{downstream} Python package and at \url{https://github.com/mmore500/downstream}, where we plan to conduct longer-term, end-user-facing development and maintenance \citep{moreno2024downstream}.
Supplemental materials are available via the Open Science Framework at \url{https://osf.io/na2wp} and \url{https://osf.io/kjpqu/} \citep{foster2017open}.
All accompanying software and materials are provided open-source under the MIT License.

This project benefited significantly from open-source scientific software \citep{2020SciPy-NMeth,harris2020array,reback2020pandas,mckinney-proc-scipy-2010,waskom2021seaborn,hunter2007matplotlib,moreno2023teeplot}.

\subsection{Benchmark Results}

\input{fig/pico-performance}

Figure \ref{fig:pico-performance} compares per-item ingest times among benchmarked approaches.
Timings were qualitively consistent across surveyed buffer sizes and data types, with the notable exception of the \textit{zhao steady} approach --- which pessimizes considerably for large buffer size.
In light of this broad consistency, quoted timings focus on the case of single-byte data with buffer capacity of 256 items.
Supplementary Table \ref{tab:TODO} provides full timing details for other benchmark conditions.

As expected, in all cases the control \textit{discard-only} and \textit{simple ringbuf} treatments achieved the fastest observed timings --- TODO ns and TODO ns, respectively.
Behind the control treatments, the naive \textit{steady doubling} and \textit{tilted doubling} algorithms produced the next-fastest timings, with ingests on the order of TODO ns and TODO ns, respectively.
Third-fastest were generalized ring buffer approaches, with the \textit{gunther steady} algorithm elapsing TODO ns per item ingested and the \textit{dstream tilted} algorithm requiring TODO ns per item ingested.
Finally, stateful, iterative approaches produced the slowest timings --- TODO ns and TODO ns.
(Note, though, that the \textit{zhao steady} algorithm did produce comparable timings to the generalized ring buffer approaches at the smallest 64-item buffer capacity).

\input{fig/steady-mem}
\input{fig/tilted-mem}

Memory usage results are shown in Figures \ref{fig:steady-mem} and \ref{fig:tilted-mem}.
In the average case, naive doubling approaches occupied between TODO\% and TODO\% larger memory footprints per item stored compared to the corresponding generalized ring buffer approach.
For stateful, iterative approaches, per-item memory footprints were between TODO\% and TODO\% larger compared to the generalized ring buffer approach.

\input{fig/steady-qos}
\input{fig/tilted-qos}

Finally, Figures \ref{fig:steady-qos} and \ref{fig:tilted-qos} compare surveyed approaches' curation quality in achieving target steady or tilted retention.
In both cases, naive doubling strategies perform worst in maintaining samples representational over stream history.
For steady curation, the \textit{zhao steady} and \textit{gunther steady} approaches produce very similar quality outcomes.
However, where targeting tilted curation, the \textit{dstream tilted} approach provides substantially lower recency-proportional gap sizes compared to \textit{zhao tilted}.
This can be understood as a consequence of the generalized ring buffer approach's efficacy in explicitly filling all available buffer space, which is not the case for the \textit{zhao tilted} approach.

In parsing these results, two key observations can be made with regard to practical trade-offs among surveyed approaches.
First, we find generalized ring buffer approaches to essentially be strictly preferable compared to stateful, iterative approaches used in current practice.
For both steady and tilted retention, generalized ring buffer approaches provide equivalent or improved curation quality, equivalent or enhanced stream processing speed, and improved memory efficiency.
Second, benchmarks indicate a speed-quality trade-off between naive doubling approaches and generalized ring buffer approaches.
Again, for both steady and tilted retention, generalized ring buffer approaches support denser, more representative sampling within a given memory footprint --- but at a cost of between TODO\% and TODO\% lower stream processing throughput.
