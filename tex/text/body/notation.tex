\section{Preliminaries, Notations, and Terminology} \label{sec:notation}

\input{fig/ingest-and-lookup.tex}

The core function of proposed algorithms is curation of incoming data on a rolling basis to dynamically filter out a subset that, according to a desired \textbf{coverage criterion}, minimizes \textbf{time gaps} between retained data items.
Incoming data is considered as a \textbf{data stream}, comprised of a one-dimensional sequence of \textbf{data items} $v_i$.
As such, we term this problem as the \textbf{data stream curation problem}.
We refer to the act of reading an item from the data stream as \textbf{ingesting} it.
For the purposes of present discussion, we assume data items to be fixed-size and do not consider the actual values of the data items further.

We will notate the \textit{number of available buffer sites} to store retained data items as $\colorS$.
Proposed algorithms assume a fixed budget of buffer space, sufficient to store $\colorS$ data items.%
\footnote{%
In associated materials, the fixed-size buffer used to store curated data items is referred to as a ``surface.''
Other related work has considered the stream curation problem with extensible memory capacity \citep{moreno2024algorithms}.%
}
They require buffer size to be an even power of two, so that for some $\colors$, $\colorS = 2^{\colors}$.
On occasion it will become necessary to refer to a specific position $\colork$ within a buffer.
We will take a zero-indexing convention so $0 \leq \colork < \colorS$.

We will refer to each \textit{data item's stream sequence index} as its ingestion time $\colorTbar$.
In referring to time, we will also take a zero-indexing convention, describing that the first element of the data stream associates to ingestion time $\colorTbar=0$.
We assume $\colorT$ to be known for each data ingestion, which is simple to accomplish by maintaining a counter that increments each time a data item is ingested.
Because we are only concerned with the sequence order of data items, and not the values of data items themselves, we will sometimes shorthand $\colorTbar$ to refer to $v_{\colorTbar}$, the data item ingested at that time.

We consider only one update operation on the buffer: storage of an ingested data item at a buffer site $\colork$.
As a \textit{simplifying limitation}, algorithms do not move, swap, or read already-stored data.
As such, the only way to discard a stored data item is to overwrite it by placing a new data item at that site.
Under this scheme, control of what data is retained and for how long occurs solely as a consequence of \textit{ingestion site selection}.
The site selection operation, schematized in Figure \ref{fig:surface-site-ingest}), therefore comprises the primary algorithmic focus of this work.

As a space-saving optimization, we store only the data items themselves in buffer space --- no metadata (e.g., ingestion time) or data structure components (e.g., indices or pointers) are stored.
(This optimization is critical, in particular, when data items are small --- such as single bits or single bytes \citep{moreno2022hereditary}.)
Note, though, that, without metadata, identifying stored data items requires capability deduce ingestion time solely from buffer position.
Figure \ref{fig:ingest-rank-calculation} depicts an example \textit{ingested-time calculation} operation.

\textit{Coverage criteria} for retained data items considered here are defined solely in terms of items' time indices $\colorTbar$ --- not their data values.
We define coverage criteria in terms of \textbf{gap size} between retained data items, taken as the number of temporally sequential data items discarded (i.e., overwritten) between when two retained items were ingested.
Let $\colorB_{\colorT}$ denote the set of data items retained in buffer at time $\colorT$ (including $v_{\colorT}$) and $\colorBnot_{\colorT}$ refer to the set of data items discarded at that time.
With these formalisms established, we can now explicitly define gap size at record index $\colorTbar$, $\colorg = \colorG_{\colorT}(\colorTbar)$.
For any $0 \leq \colorTbar \leq \colorT$,
\begin{align*}
\colorG_{\colorT}(\colorTbar) = \left| \{\colorTbar - m, \ldots, \colorTbar, \ldots, \colorTbar+n\} \in \colorBnot_{\colorT} \right|_\mathrm{max}.
\end{align*}
Note that if $\colorTbar \in \colorB_{\colorT}$, then $\colorG_{\colorT}(\colorTbar) = 0$

Algorithms for three primary coverage criteria are treated: \textbf{steady} (Section \ref{sec:steady}), \textbf{stretched} (Section \ref{sec:stretched}), and \textbf{tilted} (Section \ref{sec:tilted}).
The \textit{steady criterion} seeks to maintain data items with time indices that evenly cover elapsed history.
The \textit{stretched criterion} also seeks to maintain data items with time indices spread across elapsed history, but with proportionally-decaying density such that more early data items are maintained.
The third coverage criterion, \textit{tilted coverage}, mirrors the stretched criterion, except that more-recent data items are prioritized.
Under this criterion, recency-proportional gap size is to be minimized.
Figure \ref{fig:criteria-intuition} commpares example ideal retention distributions under each criterion.
Formal definitions are provided in its criterion's corresponding section.

It happens that for proposed stream curation algorithms algorithms, the binary order of magnitude of the time $\colorT$ plays an important conceptual role.
We will refer to this value $\colort \sim \left\lfloor \log_2(\colorT) \right\rfloor$ as the \textbf{epoch} of $\colorT$.
We associate time $\colorT=0$ to epoch $\colort=0$ and treat $\colorT < \colorS$ --- before the buffer fills --- as epoch 0.

For the \textit{steady algorithm}, epoch transition occurs immediately before a new binary order of magnitude (e.g., between $\colorT = 15$ and $\colorT = 16$ or between $\colorT = 31$ and $\colorT = 32$), so $\colort = \left\lfloor \log_2(\colorT) \right\rfloor - \colors + 1$ for $\colorT \geq \colorS$.
For the \textit{stretched} and \textit{tilted algorithms}, epoch transition occurs immediately \textit{after} a new binary order of magnitude (e.g., between $\colorT = 16$ and $\colorT = 17$).
So, in this context $\colort = \left\lfloor \log_2(\colorT - 1) \right\rfloor - \colors + 1$ for $\colorT > \colorS$.

\input{fig/hanoi-intuition.tex}

As shown in Figure \ref{fig:hanoi-intuition} --- and discussed in greater detail later on --- it happens that the structure of proposed algorithms heavily rely on abstractions based around data items' ingestion time $\colorTbar$'s position in the integer sequence $\colorH(\colorTbar) = \max \{ n \ni 2^n \mid \colorTbar \}$.%
\footnote{%
Note that, in practice, $\colorH(\colorTbar)$ can be calculated fast $\mathcal{O}(1)$ using bit-level operations --- e.g., in Python \texttt{(~T \& T-1).bit\_length()} \citep{oeis}.
}
The first few terms of this sequence are $0,\allowbreak 1,\allowbreak 0,\allowbreak 2,\allowbreak 0,\allowbreak 1,\allowbreak 0,\allowbreak 3,\allowbreak 0,\allowbreak 1,\allowbreak 0,\allowbreak 2,\allowbreak 0,\allowbreak 1,\allowbreak 0,\allowbreak 4,\allowbreak 0,\allowbreak \ldots$ \, .
This sequence appears as A007814 in the On-Line Encyclopedia of Integer Sequences \citep{oeis}.
We adopt a zero-indexing convention, so $\colorH(0) = 0$, $\colorH(1) = 1$, $\colorH(2) = 0$, etc.

For each time point $\colorT$ we refer to its corresponding entry in OEIS Sequence A007814 $\colorH(\colorT) = \colorh$ as its ``\textbf{hanoi value},'' in reference to the famous ``tower of hanoi'' puzzle \citep{lucas1889jeux}.
On occasion, we abbreviate this term as ``h.v.''
Some intuition for the structure of this sequence will be condusive to later discussions.
As depicted in Figure \ref{fig:hanoi-intuition}, the hanoi sequence exhibits recursively-nested fractal structure.
Element 0 appears every 2nd entry, element 1 appears every 4th entry, and in the general case element $\colorh$ appears every $2^{\colorh+1}$th entry.
So, a hanoi value $\colorh$ appears roughly twice as often as value $\colorh + 1$.
Remark, though, that when hanoi value $\colorh$ appears for the first time, the value $\colorh - 1$ has appeared exactly once.
So, we have seen exactly one instance of $\colorh$ and also exactly one instance of $\colorh - 1$.
At this point, the value $\colorh - 2$ has appeared exactly twice and, in general, the value $\colorh - n$ has appeared $2^{n - 1}$ times for $n > 0$.

As a final piece of minutiae, let the binary floor of a value $x$ be denoted $\left\lfloor x \right\rfloor_\mathrm{bin} = \exp(2, \left\lfloor \log_2 x \right\rfloor)$.
