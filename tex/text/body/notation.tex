\section{Preliminaries, Notations, and Terminology} \label{sec:notation}

\input{fig/ingest-and-lookup.tex}

The core function of proposed algorithms is curation of incoming data on a rolling basis to dynamically filter out a subset that, according to a desired \textbf{coverage criterion}, minimizes \textbf{time gaps} between retained data items.
Incoming data is considered as a \textbf{data stream}, comprised of a one-dimensional sequence of \textbf{data items} $v_i$.
As such, we term this problem as the \textbf{data stream curation problem}.
We refer to the act of reading an item from the data stream as \textbf{ingesting} it.
For the purposes of present discussion, we assume data items to be fixed-size and do not consider the actual values of the data items further.

\subsection{Buffer Storage $\colorS$}

We will notate the \textit{number of available buffer sites} to store retained data items as $\colorS$.
Proposed algorithms assume a fixed budget of buffer space, sufficient to store $\colorS$ data items.%
\footnote{%
In associated materials, the fixed-size buffer used to store curated data items is referred to as a ``surface.''
Space-efficient solutions for the stream curation problem under extensible memory capacity have been considered in other work \citep{moreno2024algorithms}.%
}
They require buffer size to be an even power of two, so that for some $\colors$, $\colorS = 2^{\colors}$.
On occasion it will become necessary to refer to a specific position $\colork$ within a buffer.
We will take a zero-indexing convention so $0 \leq \colork < \colorS$.

We consider only one update operation on the buffer: storage of an ingested data item at a buffer site $\colork$.
As a \textit{simplifying limitation}, algorithms do not move, swap, or read already-stored data.
As such, the only way to discard a stored data item is to overwrite it by placing a new data item at that site.
Under this scheme, control of what data is retained and for how long occurs solely as a consequence of \textit{ingestion site selection}.
The site selection operation, schematized in Figure \ref{fig:surface-site-ingest}), therefore comprises the primary algorithmic focus of this work.

As a space-saving optimization, we store only the data items themselves in buffer space --- no metadata (e.g., ingestion time) or data structure components (e.g., indices or pointers) are stored.
(This optimization is critical, in particular, when data items are small --- such as single bits or single bytes \citep{moreno2022hereditary}.)
Note, though, that, without metadata, identifying stored data items requires capability deduce ingestion time solely from buffer position.
Figure \ref{fig:ingest-rank-calculation} depicts an example \textit{ingested-time calculation} operation.

\subsection{Time $\colorTbar$}

We will refer to each \textit{data item's stream sequence index} as its ingestion time $\colorTbar$, and the existing number of ingested items as the \textit{current time} $\colorT$.
In referring to time, we will also take a zero-indexing convention, describing that the first element of the data stream associates to ingestion time $\colorTbar=0$.
We assume $\colorT$ to be known for each data ingestion, which is simple to accomplish by maintaining a counter that increments each time a data item is ingested.
Because we are only concerned with the sequence order of data items, and not the values of data items themselves, we will sometimes shorthand $\colorTbar$ to refer to $v_{\colorTbar}$, the data item ingested at that time.

In principle, data stream curation would support indefinite ingestions, $\colorT \in 0, 1, \ldots$.
Our proposed \textit{steady curation} algorithm, introduced below, operates in this fashion.
However, our proposed \textit{stretched} and \textit{tilted curation} algorithms accept only $2^{\colorS}$ ingestions, $\colorT \in 0, 1, \ldots 2^{\colorS} - 1$.
We expect this capacity to suffice for many applications, even with only moderately-sized buffers.
For instance, capacity for a 64-site buffer suffices to ingest items continuously at 5GHz for well over 100 years.
As such, we leave behavior for stretched and tilted curation past $2^{\colorS}$ ingests to future work.

\subsection{Gap Size $\colorg$}

\textit{Coverage criteria} for retained data items considered here are defined solely in terms of items' time indices $\colorTbar$ --- not their data values.
We define coverage criteria in terms of \textbf{gap size} between retained data items, taken as the number of temporally sequential data items discarded (i.e., overwritten) between when two retained items were ingested.
Let $\colorB_{\colorT}$ denote the set of data items retained in buffer at time $\colorT$ (including $v_{\colorT}$) and $\colorBnot_{\colorT}$ refer to the set of data items discarded at that time.
With these formalisms established, we can now explicitly define gap size at record index $\colorTbar$, $\colorg = \colorG_{\colorT}(\colorTbar)$.
For any $0 \leq \colorTbar \leq \colorT$,
\begin{align*}
\colorG_{\colorT}(\colorTbar) = \left| \{\colorTbar - m, \ldots, \colorTbar, \ldots, \colorTbar+n\} \in \colorBnot_{\colorT} \right|_\mathrm{max}.
\end{align*}
Note that if $\colorTbar \in \colorB_{\colorT}$, then $\colorG_{\colorT}(\colorTbar) = 0$.

\subsection{Coverage Criteria}

Algorithms for three primary coverage criteria are treated:
\begin{enumerate}
\item \textbf{steady criterion}: calls to maintain data items \textit{evenly covering} elapsed history (Section \ref{sec:steady}),
\item \textbf{stretched criterion}: favors \textit{older} data items proportionally to time index $\colorTbar$ (Section \ref{sec:stretched}), and
\item \textbf{tilted criteiron}: favors \textit{newer} data items proportionally to time index recency $\colorT - \colorTbar$ (Section \ref{sec:tilted}).
\end{enumerate}
Figure \ref{fig:criteria-intuition} commpares example ideal retention distributions under each criterion.
Formal definitions are provided in its criterion's corresponding section.

\subsection{Time Hanoi Value $\colorh$}

\input{fig/hanoi-intuition.tex}

As shown in Figure \ref{fig:hanoi-intuition} --- and discussed in greater detail later on --- it happens that the structure of proposed algorithms heavily rely on abstractions based around data items' ingestion time $\colorTbar$'s position in the integer sequence $\colorH(\colorTbar) = \max \{ n \ni 2^n \mid \colorTbar \}$.%
\footnote{%
Note that, in practice, $\colorH(\colorTbar)$ can be calculated fast $\mathcal{O}(1)$ using bit-level operations --- e.g., in Python \texttt{(~T \& T-1).bit\_length()} \citep{oeis}.
}
The first few terms of this sequence are $0,\allowbreak 1,\allowbreak 0,\allowbreak 2,\allowbreak 0,\allowbreak 1,\allowbreak 0,\allowbreak 3,\allowbreak 0,\allowbreak 1,\allowbreak 0,\allowbreak 2,\allowbreak 0,\allowbreak 1,\allowbreak 0,\allowbreak 4,\allowbreak 0,\allowbreak \ldots$ \, .
This sequence appears as A007814 in the On-Line Encyclopedia of Integer Sequences \citep{oeis}.
We adopt a zero-indexing convention for $\colorTbar$, so $\colorH(0) = 0$, $\colorH(1) = 1$, $\colorH(2) = 0$, etc.

For each time point $\colorT$ we refer to its corresponding entry in OEIS Sequence A007814 $\colorH(\colorT) = \colorh$ as its ``\textbf{hanoi value},'' in reference to the famous ``tower of hanoi'' puzzle \citep{lucas1889jeux}.
On occasion, we abbreviate this term as ``h.v.''
Some intuition for the structure of this sequence will be condusive to later discussions.
As depicted in Figure \ref{fig:hanoi-intuition}, the hanoi sequence exhibits recursively-nested fractal structure.
Element 0 appears every 2nd entry, element 1 appears every 4th entry, and in the general case element $\colorh$ appears every $2^{\colorh+1}$th entry.
So, a hanoi value $\colorh$ appears roughly twice as often as value $\colorh + 1$.
Remark, though, that when hanoi value $\colorh$ appears for the first time, the value $\colorh - 1$ has appeared exactly once.
So, we have seen exactly one instance of $\colorh$ and also exactly one instance of $\colorh - 1$.
At this point, the value $\colorh - 2$ has appeared exactly twice and, in general, the value $\colorh - n$ has appeared $2^{n - 1}$ times for $n > 0$.

\subsection{Time Epoch $\colort$}

Owing to h.v.-based structure, the binary order of magnitude of the time $\colorT$ plays an important conceptual role.
We will refer to this value $\colort \sim \left\lfloor \log_2(\colorT) \right\rfloor$ as the \textbf{epoch} of $\colorT$.
We associate time $\colorT=0$ to epoch $\colort=0$ and treat $\colorT < \colorS$ --- before the buffer fills --- as epoch 0.

For the \textit{steady algorithm}, epoch transition occurs immediately before a new binary order of magnitude (e.g., between $\colorT = 15$ and $\colorT = 16$ or between $\colorT = 31$ and $\colorT = 32$), so $\colort = \left\lfloor \log_2(\colorT) \right\rfloor - \colors + 1$ for $\colorT \geq \colorS$.
For the \textit{stretched} and \textit{tilted algorithms}, epoch transition occurs immediately \textit{after} a new binary order of magnitude (e.g., between $\colorT = 16$ and $\colorT = 17$).
So, in this context $\colort = \left\lfloor \log_2(\colorT - 1) \right\rfloor - \colors + 1$ for $\colorT > \colorS$.

\subsection{Miscellania}

As a final piece of minutiae, let the binary floor of a value $x$ be denoted $\left\lfloor x \right\rfloor_\mathrm{bin} = 2^{\left\lfloor \log_2 x \right\rfloor}$.
